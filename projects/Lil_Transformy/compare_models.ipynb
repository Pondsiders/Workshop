{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Model Comparison: The Evolution\n",
    "\n",
    "*From amoeba to specialized mammal.*\n",
    "\n",
    "---\n",
    "\n",
    "This notebook loads all trained models from the Lil Transformy sequence and compares their outputs on the same prompts. The complete evolutionary journey.\n",
    "\n",
    "**The lineage:**\n",
    "- 03: Bigram — the amoeba\n",
    "- 04: + Attention — eyes\n",
    "- 05: + Position — knows where it is\n",
    "- 06: + FFN — can think\n",
    "- 07: + Residuals & LayerNorm — spine\n",
    "- 08: + Stacked blocks — legs (crawled onto land)\n",
    "- 09: + Multi-head — bigger brain (mammal)\n",
    "- 10: + MoE — specialized brain regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "import math\n",
    "\n",
    "# Reproducibility for generation\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tokenizer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4,096\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "class LilTokenizer:\n",
    "    \"\"\"Compact tokenizer for Lil Transformy.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpt2_to_compact, compact_to_gpt2, vocab_size):\n",
    "        self.gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "        self.gpt2_to_compact = gpt2_to_compact\n",
    "        self.compact_to_gpt2 = compact_to_gpt2\n",
    "        self.vocab_size = vocab_size\n",
    "        self.pad_id = 0\n",
    "        self.unk_id = 1\n",
    "        self.eos_id = 2\n",
    "    \n",
    "    def encode(self, text, add_eos=True):\n",
    "        gpt2_tokens = self.gpt2_tokenizer.encode(text)\n",
    "        compact_tokens = [self.gpt2_to_compact.get(t, self.unk_id) for t in gpt2_tokens]\n",
    "        if add_eos:\n",
    "            compact_tokens.append(self.eos_id)\n",
    "        return compact_tokens\n",
    "    \n",
    "    def decode(self, token_ids):\n",
    "        gpt2_tokens = []\n",
    "        for tid in token_ids:\n",
    "            if tid in [self.pad_id, self.unk_id, self.eos_id]:\n",
    "                continue\n",
    "            if tid in self.compact_to_gpt2:\n",
    "                gpt2_tokens.append(self.compact_to_gpt2[tid])\n",
    "        return self.gpt2_tokenizer.decode(gpt2_tokens)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.vocab_size\n",
    "    \n",
    "    @classmethod\n",
    "    def load(cls, path):\n",
    "        with open(path, 'r') as f:\n",
    "            config = json.load(f)\n",
    "        gpt2_to_compact = {int(k): v for k, v in config['gpt2_to_compact'].items()}\n",
    "        compact_to_gpt2 = {int(k): v for k, v in config['compact_to_gpt2'].items()}\n",
    "        return cls(gpt2_to_compact, compact_to_gpt2, config['vocab_size'])\n",
    "\n",
    "\n",
    "tokenizer = LilTokenizer.load('tokenizer/tokenizer.json')\n",
    "VOCAB_SIZE = len(tokenizer)\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "models-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Model Definitions\n",
    "\n",
    "We need to define each model architecture to load the weights. This will grow as we add more notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "model-03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 03: Bigram ===\n",
    "\n",
    "class BigramLM(nn.Module):\n",
    "    \"\"\"Notebook 03: Simplest autoregressive model. Each position predicts next from itself only.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.unembed(self.embedding(x))\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                # Bigram only looks at last token\n",
    "                x = torch.tensor([[tokens[-1]]], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, 0] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:  # EOS\n",
    "                    break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "model-04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 04: Attention ===\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    \"\"\"Single-head causal self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.scale = math.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        Q, K, V = self.W_q(x), self.W_k(x), self.W_v(x)\n",
    "        scores = (Q @ K.transpose(-2, -1)) / self.scale\n",
    "        scores = scores.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        return self.W_o(attn @ V)\n",
    "\n",
    "\n",
    "class AttentionLM(nn.Module):\n",
    "    \"\"\"Notebook 04: Bigram + single-head attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.unembed(self.attention(self.embedding(x)))\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-256:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 05: Attention + Position ===\n",
    "\n",
    "class PositionalAttentionLM(nn.Module):\n",
    "    \"\"\"Notebook 05: Attention + learned positional embeddings.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        return self.unembed(self.attention(tok_emb + pos_emb))\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 06: Attention + Position + FFN ===\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"Position-wise feedforward network.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(F.relu(self.linear1(x)))\n",
    "\n",
    "\n",
    "class AttentionFFNLM(nn.Module):\n",
    "    \"\"\"Notebook 06: Attention + Position + FFN.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        attended = self.attention(tok_emb + pos_emb)\n",
    "        processed = self.ffn(attended)\n",
    "        return self.unembed(processed)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 07: Transformer Block (Residuals + LayerNorm) ===\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"A single transformer block with pre-norm architecture.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attention = CausalSelfAttention(d_model, max_seq_len)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLM(nn.Module):\n",
    "    \"\"\"Notebook 07: Proper transformer block with residuals and LayerNorm.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.block = TransformerBlock(d_model, d_ff, max_seq_len)\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        x = self.block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.unembed(x)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "model-placeholders",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architectures defined.\n"
     ]
    }
   ],
   "source": [
    "# === 08: Stacked Blocks ===\n",
    "\n",
    "class StackedTransformerLM(nn.Module):\n",
    "    \"\"\"Notebook 08: Multiple transformer blocks stacked.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_layers, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(d_model, d_ff, max_seq_len)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.unembed(x)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 09: Multi-Head Attention ===\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"Multi-head causal self-attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_k = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_v = nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_o = nn.Linear(d_model, d_model, bias=False)\n",
    "        \n",
    "        mask = torch.triu(torch.ones(max_seq_len, max_seq_len), diagonal=1).bool()\n",
    "        self.register_buffer('mask', mask)\n",
    "        self.scale = math.sqrt(self.head_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        K = self.W_k(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        V = self.W_v(x).view(B, T, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        scores = (Q @ K.transpose(-2, -1)) / self.scale\n",
    "        scores = scores.masked_fill(self.mask[:T, :T], float('-inf'))\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        out = attn @ V\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        return self.W_o(out)\n",
    "\n",
    "\n",
    "class MultiHeadTransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with multi-head attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, max_seq_len)\n",
    "        self.ffn = FeedForward(d_model, d_ff)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.ffn(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiHeadTransformerLM(nn.Module):\n",
    "    \"\"\"Notebook 09: Full transformer with multi-head attention.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MultiHeadTransformerBlock(d_model, n_heads, d_ff, max_seq_len)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.unembed(x)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "# === 10: Mixture of Experts ===\n",
    "\n",
    "class MoELayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Mixture of Experts layer with top-1 routing.\n",
    "    Each token is routed to exactly one expert.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, d_ff=None, n_experts=2):\n",
    "        super().__init__()\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.n_experts = n_experts\n",
    "        self.d_model = d_model\n",
    "        self.experts = nn.ModuleList([\n",
    "            FeedForward(d_model, d_ff) for _ in range(n_experts)\n",
    "        ])\n",
    "        self.router = nn.Linear(d_model, n_experts, bias=False)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T, D = x.shape\n",
    "        router_logits = self.router(x)\n",
    "        router_probs = F.softmax(router_logits, dim=-1)\n",
    "        expert_indices = router_probs.argmax(dim=-1)\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=2)\n",
    "        indices_expanded = expert_indices.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, -1, D)\n",
    "        output = expert_outputs.gather(dim=2, index=indices_expanded).squeeze(2)\n",
    "        return output\n",
    "\n",
    "\n",
    "class MoETransformerBlock(nn.Module):\n",
    "    \"\"\"Transformer block with Mixture of Experts instead of dense FFN.\"\"\"\n",
    "    \n",
    "    def __init__(self, d_model, n_heads, d_ff=None, n_experts=2, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, n_heads, max_seq_len)\n",
    "        self.moe = MoELayer(d_model, d_ff, n_experts)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.attention(self.ln1(x))\n",
    "        x = x + self.moe(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class MoETransformerLM(nn.Module):\n",
    "    \"\"\"Notebook 10: Transformer with Mixture of Experts.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, d_model, n_layers, n_heads, n_experts=2, d_ff=None, max_seq_len=256):\n",
    "        super().__init__()\n",
    "        self.max_seq_len = max_seq_len\n",
    "        if d_ff is None:\n",
    "            d_ff = 4 * d_model\n",
    "        \n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.blocks = nn.ModuleList([\n",
    "            MoETransformerBlock(d_model, n_heads, d_ff, n_experts, max_seq_len)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "        self.unembed = nn.Linear(d_model, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B, T = x.shape\n",
    "        tok_emb = self.token_embedding(x)\n",
    "        pos_emb = self.position_embedding(torch.arange(T, device=x.device))\n",
    "        x = tok_emb + pos_emb\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        x = self.ln_final(x)\n",
    "        return self.unembed(x)\n",
    "    \n",
    "    def generate(self, prompt_tokens, max_new_tokens=50, temperature=1.0):\n",
    "        self.eval()\n",
    "        tokens = list(prompt_tokens)\n",
    "        generated = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(max_new_tokens):\n",
    "                context = tokens[-self.max_seq_len:]\n",
    "                x = torch.tensor([context], device=next(self.parameters()).device)\n",
    "                logits = self.forward(x)\n",
    "                probs = F.softmax(logits[0, -1] / temperature, dim=-1)\n",
    "                next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "                \n",
    "                generated.append(next_token)\n",
    "                tokens.append(next_token)\n",
    "                \n",
    "                if next_token == 2:\n",
    "                    break\n",
    "        \n",
    "        return generated\n",
    "\n",
    "\n",
    "print(\"Model architectures defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loading-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Load Available Models\n",
    "\n",
    "Check which checkpoints exist and load them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "load-models",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n",
      "============================================================\n",
      "✓ 03: Bigram\n",
      "    Parameters: 1,052,672\n",
      "    Final perplexity: 35.8\n",
      "✓ 04: + Attention\n",
      "    Parameters: 1,118,208\n",
      "    Final perplexity: 25.0\n",
      "✓ 05: + Position\n",
      "    Parameters: 1,150,976\n",
      "    Final perplexity: 17.7\n",
      "✓ 06: + FFN\n",
      "    Parameters: 1,282,688\n",
      "    Final perplexity: 13.4\n",
      "✓ 07: + Residual\n",
      "    Parameters: 1,283,456\n",
      "    Final perplexity: 10.9\n",
      "✓ 08: + 2 Blocks\n",
      "    Parameters: 1,481,216\n",
      "    Final perplexity: 8.7\n",
      "✓ 09: + 2 Heads\n",
      "    Parameters: 1,481,216\n",
      "    Final perplexity: 8.3\n",
      "✓ 10: + MoE\n",
      "    Parameters: 1,745,152\n",
      "    Final perplexity: 8.1\n",
      "\n",
      "Loaded 8 models.\n"
     ]
    }
   ],
   "source": [
    "# Registry of models: (checkpoint_file, model_class, display_name, extra_kwargs, uses_n_layers, uses_n_heads, uses_n_experts)\n",
    "MODEL_REGISTRY = [\n",
    "    ('03_bigram.pt', BigramLM, '03: Bigram', {}, False, False, False),\n",
    "    ('04_attention.pt', AttentionLM, '04: + Attention', {}, False, False, False),\n",
    "    ('05_positional.pt', PositionalAttentionLM, '05: + Position', {'max_seq_len': 256}, False, False, False),\n",
    "    ('06_ffn.pt', AttentionFFNLM, '06: + FFN', {'max_seq_len': 256}, False, False, False),\n",
    "    ('07_transformer_block.pt', TransformerLM, '07: + Residual', {'max_seq_len': 256}, False, False, False),\n",
    "    ('08_stacked.pt', StackedTransformerLM, '08: + 2 Blocks', {'max_seq_len': 256}, True, False, False),\n",
    "    ('09_multihead.pt', MultiHeadTransformerLM, '09: + 2 Heads', {'max_seq_len': 256}, True, True, False),\n",
    "    ('10_moe.pt', MoETransformerLM, '10: + MoE', {'max_seq_len': 256}, True, True, True),\n",
    "]\n",
    "\n",
    "models = {}\n",
    "stats = {}\n",
    "\n",
    "print(\"Loading models...\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for checkpoint_file, model_class, name, extra_kwargs, uses_n_layers, uses_n_heads, uses_n_experts in MODEL_REGISTRY:\n",
    "    path = Path(checkpoint_file)\n",
    "    if path.exists():\n",
    "        checkpoint = torch.load(path, map_location='cpu', weights_only=False)\n",
    "        \n",
    "        # Build kwargs from checkpoint\n",
    "        kwargs = {'vocab_size': checkpoint['vocab_size'], 'd_model': checkpoint['d_model']}\n",
    "        kwargs.update(extra_kwargs)\n",
    "        if 'd_ff' in checkpoint:\n",
    "            kwargs['d_ff'] = checkpoint['d_ff']\n",
    "        if uses_n_layers and 'n_layers' in checkpoint:\n",
    "            kwargs['n_layers'] = checkpoint['n_layers']\n",
    "        if uses_n_heads and 'n_heads' in checkpoint:\n",
    "            kwargs['n_heads'] = checkpoint['n_heads']\n",
    "        if uses_n_experts and 'n_experts' in checkpoint:\n",
    "            kwargs['n_experts'] = checkpoint['n_experts']\n",
    "        \n",
    "        # Create model\n",
    "        model = model_class(**kwargs)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        models[name] = model\n",
    "        stats[name] = {\n",
    "            'params': sum(p.numel() for p in model.parameters()),\n",
    "            'final_ppl': checkpoint['history']['val_perplexity'][-1]\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {name}\")\n",
    "        print(f\"    Parameters: {stats[name]['params']:,}\")\n",
    "        print(f\"    Final perplexity: {stats[name]['final_ppl']:.1f}\")\n",
    "    else:\n",
    "        print(f\"✗ {name} (not found: {checkpoint_file})\")\n",
    "\n",
    "print()\n",
    "print(f\"Loaded {len(models)} models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Side-by-Side Generation\n",
    "\n",
    "Give all models the same prompt, see what they produce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "compare-function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_generations(prompt, max_tokens=50, temperature=1.0, seed=None):\n",
    "    \"\"\"\n",
    "    Generate from all loaded models with the same prompt.\n",
    "\n",
    "    If seed is provided, it's combined with a hash of the prompt so that:\n",
    "    - Same prompt + same seed = reproducible across runs\n",
    "    - Different prompts + same seed = different outputs (not stuck in attractors)\n",
    "    - All models for a given prompt get the same seed (fair comparison)\n",
    "    \"\"\"\n",
    "    prompt_tokens = tokenizer.encode(prompt, add_eos=False)\n",
    "\n",
    "    # Combine seed with prompt hash so different prompts get different randomness\n",
    "    if seed is not None:\n",
    "        prompt_seed = seed + hash(prompt) % 10000\n",
    "    else:\n",
    "        prompt_seed = None\n",
    "\n",
    "    for name, model in models.items():\n",
    "        if prompt_seed is not None:\n",
    "            torch.manual_seed(prompt_seed)\n",
    "\n",
    "        generated = model.generate(prompt_tokens, max_new_tokens=max_tokens, temperature=temperature)\n",
    "        text = tokenizer.decode(generated)\n",
    "\n",
    "        ppl = stats[name]['final_ppl']\n",
    "        print(f\"{name} (ppl={ppl:.1f}):\")\n",
    "        print()\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated: {text}\")\n",
    "        print()\n",
    "        print(\"-\" * 70)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "classic-prompts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# CLASSIC PROMPTS\n",
      "######################################################################\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated: . They saw his spaceship. He had no money on leaves, Tom tried to climb. They measured the door fell out the first time, he had never found a big crystal and are a big secret in the bathe.\" They ran to shoot\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated: . So, there was a little girl named Lily. One day Tom tried to climb. But, she asked her mother him first time, she decided to catch it again, her mom if she started to jump alligator, she laughed. The train\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated: , there was a smart frog named Timmy. Timmy loved to explore such a big castle. He always remember what him grew dark and he had never found a shiny crystal and that a thing was in the warm spot. \n",
      "\n",
      "L\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated: , there was a little girl named Mia. She loved to play outside in the sunshine. One day, she fell out and up into her window and the garden had a nice meal.\n",
      "\n",
      "\"What are you two best!\" the sun started train\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated:  there was a thick rose that fit in the forest. One day, a little bird was chubby and he fell out of his nest. Big the nest had saved the worm. He was very excited and he had made a new friend.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated:  there was a thick island. He had no money in the sea. One day the island came an island. The island was dark and gray. The island was sad. The island was independent and didn't want to bathe. But no one could\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated: , there was a little girl named Lily. She loved to play outside in the sunshine. One day, she fell and hurt her leg. Her mom came and had a bad face. Lily cried and cried in all her leg. Her mom saw her\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: Once upon a time\n",
      "Generated:  there was a thick forest. He had no looking for leaves, but he was still. Suddenly, the tree fell out of his hands and starting to get a see. It was and a big secret in the tree.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated: : \"Let's room and Sue did not understand. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Her mom said they were very clever he hit Ben. You have the wind costume and said, Lily saw a loud noises and playing with. They wanted to play with\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated:  She was always careful and put it on the ground. From day on, she loved balloons on, the ground.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated:  her mommy. It was very gifted and found in the drawer a book. It was dark and the living room. It hit Ben. You have the wind blew the doll hard and quickly slid down. He had many. He wanted to play with\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated:  big saw the pebble and the fence was in the dirty floor.\n",
      "\n",
      "The girl said, \"R, I recommend you squeeze this b. Do you understand?\"\n",
      "\n",
      "The girl said \"Yes!\" thought and then, \"\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated:  had a shiny necklace for her home with her hands in the dirty box.\n",
      "\n",
      "Her mom said they were very clever. There was toy cup the wind in the middle of her garden and their way to ride the veil to the, where\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated: : \"Let you speed it home with me!\". \n",
      "Every day, she wanted to be good! She loved learning new things and never tried to b costume around the creature who saw it flying in the window. She thought it was great.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated:  flew up to the girl and said with a smile in delight and joy.\n",
      "\n",
      "The girl said they were very clever and that she wanted to keep the wind. She said they could reach and admire them often. The little girl wanted each again,\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: The little girl\n",
      "Generated:  flew onto the shoe and explored home with her hands in the sun.\n",
      "\n",
      "The little girl was the open field all day that she wanted to keep the wind safe. Then her tummy slid back into the field and lay down on the sidewalk.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad.\n",
      "One day long he had learnt a little bird flew away open for the be fun! Can you!\" The lady who liked to be left the top, happy. He likes Timmy, but be kind and challenge. He tried to play\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad because they have a long for you eat some vegetables now married!\" the first, he could enjoy their adventures together again. They are wild pig left the top, happy. He had big for a little animal. The challenge.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad and they saw a hole. \"This is very old, let open for the hole it! Can you help me?\" Sara asks Tom. He has he says, \"Yes, Tim. looks like it. We don't want to sleep that\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad.\n",
      "\n",
      "\"Don't worry, I can show me how open for the field couch! We can make a face.\" Sam says. He has an sand with a bag. Tim and looks like shiny.\n",
      "\n",
      "\"Let's go there\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad.\n",
      "\n",
      "\"Where did you see?\" asked the secret.\n",
      "\n",
      "Lila replied, \"It is thunder! I'm going to freeze the top, here!\" Lila replied, \"No, why don't you're not so bad\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad.\n",
      "\n",
      "\"Bob, why is I like you dad? He is getting more magic! Can you repair your toys?\" Bob asked. He has a big hug. He likes Tim and I like to make you happy. He is sorry that\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad with a toy into a sad seat. But then he heard a open sound. He looked around and saw a boy standing in a wild corner of the room. The boy said, \"Wow, the 3 year old here. What is this?\"\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: He was very\n",
      "Generated:  sad.\n",
      "\n",
      "\"Where did you eat?\" asked the dad.\n",
      "\n",
      "Lila and Max petted Grandma. They ate candy. It was yummy and funny.\n",
      "\n",
      "\"Go away, kids. This time, grab this special\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The classics\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# CLASSIC PROMPTS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "compare_generations(\"Once upon a time\", seed=42)\n",
    "compare_generations(\"The little girl\", seed=42)\n",
    "compare_generations(\"He was very\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "story-prompts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# STORY STARTERS\n",
      "######################################################################\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  made something was sad, leaving the magical voice.\n",
      "The butterfly that sometimes we have such a kind to help?\"\n",
      "Lily felt cool. That is not bite. They are faster again soon became dark, and stared. One day on their bikes\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  loved to sit on a helpful friend didn't know what they are having so much fun. One day, Sarah said, \"I want you did not think he doing. But remember to run away. In, and still. Can we share their bikes\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  loved her aunt's mom provided her treat. She gave her some tomatoes to help, so she wiped her help her mommy. Her dad did not think the little boy went and faster again soon. In, there a girl named Amy. Amy was\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  loved to sit on, especially her magical clean shelves. One day, she saw a bird singing. Tweet was eating some yummy lemonade. \n",
      "\n",
      "Lily was hungry and wanted to help Poppy and he jumped, but it didn't\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  had something that she loved with her magical powers. One day, her mom took her outside and got to help. Lily tried to open the box and found the box. Lily was very upset and wished she had not to take her a toy. She\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  had something very special, she loved to ride her bike. One day, she saw a drop on her bike. It had bright blue and red eyes and Lily wanted to ride it faster.\n",
      "\n",
      "But, she did not come right. She wanted\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  had long hair on a chair that she loved very much. One day, she saw a butterfly flying high in the sky! She wanted to show it and believe in her room.\n",
      "\n",
      "When she got, she stared at the butterfly and wanted to\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl named Lily. She\n",
      "Generated:  had something that she loved to dress up. One day, Lily's mom took her outside to the park. Lily saw a bright mailbox and wanted to join the big tree. She picked it up and started to climb up.\n",
      "\n",
      "As she was\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  perfect thing by was in a boy walking.\n",
      "Tommy.\n",
      "\n",
      "\n",
      "\n",
      "\" carefully reached the junk outside when it tasted so much they had helped the tree. Lily. She said to make his feelings and make her name and get dark\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  perfect thing by was in a pet bird.\n",
      "The dog. He was too rough. It could do the cat looked at meowled again. Soon the tree. Lily could still, saw a balloon. Please, dog just go home.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  sad, and from it wasn't frustrated.\n",
      "\n",
      "One day, Miss Lisa's mom carefully reached for the cat. She saw an image by a toe with a magical grill. She said to the door. She was her little brother, \"\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  small, and from then on top of the flea wind. \n",
      "\n",
      "\"Oh no, Lily, but it's time to go!\" cried her mummy. \"I know, it's time to sleep.\" her little brother did not\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  small thing that smelled really good.\n",
      "\n",
      "One day, the cat heard a loud noise. It was Max had a mean cat! He looked around and saw the wet. Lily was curious, but he didn't know what to do.\n",
      "\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  barking. The was very loud and scared.\n",
      "\n",
      "One day, the bigger dog heard a loud scream. The mean dog stopped fighting and looked at the sheep. It saw the man, saw the raven. He was scared. He did not\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  small. The was very curious. They liked to take things with the dog and take the nap. They had a new friend, a secret camera. They learned to be kind to each other.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: The big dog and the small cat were\n",
      "Generated:  sad. After from in a big house, the cow, the cow was too rough. She could do the cat but it was too much to be caught.\n",
      "\n",
      "The little girl, her puppy, but the cat her name was Spot.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get far. She put cheese juice too hard. She wanted to learn to cry, she decided to them each other chalk.\n",
      "\n",
      "Timmy and asked if the parrot.\n",
      "When they rode the children. They like \"Of course,\" Lily\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get money. A stand up in the new toy car for a curious and brings back and the model accidently carefully wrap. They forgot about the light!\n",
      "\n",
      "\n",
      "\n",
      "Sara says, \"Give me your sack and I will show it!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get far away. It was a flower that on its little girl dared to stay in the garden. When it was done, he was very busy and asked another rabbit, birds, and bringing the gray bird to sing until it was a duck. Lily\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get. The kids stand up in the park. The dog was curious and asked, \"Do you want to see?\" The kids was big and patient!\n",
      "\n",
      "The birds saw that the birds had made of bees, and squirrel running. The friends\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get to the car stand up. \n",
      "\n",
      "Welcome home, Lily and Timmy was so happy. He carefully carried the car back home. Lily was excited too. She saw that Tommy had told her to be careful and not running. Lily ran\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  play. The kids wanted to get the toy. The dog said, \"Be great and fun, too!\" The children were so happy. They walked to another park and said, \"Hi, dog! You are a good dog. You make friends\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get. The kids put on their shoes and ran out the door. But there was no one. The little kids were scared, so they decided to collect another pebble.\n",
      "\n",
      "But the little children had a lot of people. They ran\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: One sunny day, the children went to the park to\n",
      "Generated:  get some green chalk stand.\n",
      "\n",
      " a man came outside and dared to mail the chalk to mail them to the chalk. The kids were very happy!\n",
      "\n",
      "They bought some stuff for the friends. They cleaned it and placed the chalk on them\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Story starters\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# STORY STARTERS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "compare_generations(\"Once upon a time there was a little girl named Lily. She\", seed=42)\n",
    "compare_generations(\"The big dog and the small cat were\", seed=42)\n",
    "compare_generations(\"One sunny day, the children went to the park to\", seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "challenging-prompts",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "######################################################################\n",
      "# CONTEXT-DEPENDENT PROMPTS\n",
      "######################################################################\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys. Buzz was thick box on, mom if they opened his mom said.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys instead picking things to always ask her mom if they could go inside. One day, they say they see the house.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys. She put on her favourite toy mom and saw that his toy car a toy. She had a turn on the lid.\n",
      "\n",
      "\"Oh, no!\" Mia tried it had made an true. It made bread the meat on the side of\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys and sing songs. She dreamed of mom and dad, and they play a different game.\n",
      "\n",
      "Later, Lily saw a butterfly on the butterfly and flew through the window, had made of true. It was a flower, and people worked\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys. She would roll it on her mom's bed, and her lip.\n",
      "\n",
      "One day, she was playing in the playground, feeling a nice day with a new video. She could hear that everyone in her neighborgy dress and knew that\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  red ball, but she always looked up at it every time she sat on a different side.\n",
      "\n",
      "One day, she decided to go to the park. She thought of a way to climb with her friends. She started to run towards the park\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys. Ben would roll his ball to mom and look for his mom. He had a red ball that she loved. But she also wanted to find his ball with his friend, Tom, but she could not find her ball. She was troubled and\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: Lily had a red ball. She loved to play with her\n",
      "Generated:  toys. She would roll it on her mom's lap, and dance. It was fun and had a turn on her own.\n",
      "\n",
      "One day, Lily's mom tried to eat the ball with her mouth. Lily didn't like that. She\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated: . Now, and looked around her mommy played at Jimmy was very brave knight came in the wind outside. She said, \" saw a new things!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated:  \"I don't need help me.\" Timmy ran to catch it. They never forget. He was ignorant and hugged him.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated:  wrong, Lily and Tim. He looked at the people by in. They said, \"We are ignorant and not sing happy. I are bad things! You have to have fun. You are so clean the car. Now we can go and\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated: , \"You have to bathe my cat. You can get some carrots. That is a gift from happening and happiness to me,\" he said. \"You are not being gentle with sharp.\"\n",
      "\n",
      "The teacher smiled and replied, \"Don't\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated:  he would make it look pretty and maybe they played all day long.\n",
      "\n",
      "Once he said, \"Let's take a break dive?\" His mom smiled and said, \"Yes, you can play with us. But we have to do something you\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated: , \"It's not your car, Tim. You can't fix. You must have said, and be kind with you. But I are just curious about history things.\"\n",
      "\n",
      "Lily felt sad and took his bed from her skin. She\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated: , \"You must never buy whoâ€™t have enough money. You must have been very responsible. I will tell you something bad are about things!\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: The boy was sad because his toy was broken. His mom said\n",
      "Generated: , \"You will never spoil yourself and your toys. We can dry. But don't forget to show your toy with me.?\"\n",
      "\n",
      " nodded and said, \"Okay, Mom, thank you. You are a good soldier. I\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# More challenging - requires context\n",
    "print(\"\\n\" + \"#\" * 70)\n",
    "print(\"# CONTEXT-DEPENDENT PROMPTS\")\n",
    "print(\"#\" * 70)\n",
    "\n",
    "# These require remembering earlier context\n",
    "compare_generations(\"Lily had a red ball. She loved to play with her\", seed=42)\n",
    "compare_generations(\"The boy was sad because his toy was broken. His mom said\", seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stats-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "stats-table",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL COMPARISON\n",
      "============================================================\n",
      "\n",
      "Model                     Parameters      Perplexity  \n",
      "----------------------------------------------------\n",
      "03: Bigram                   1,052,672       35.8\n",
      "04: + Attention              1,118,208       25.0\n",
      "05: + Position               1,150,976       17.7\n",
      "06: + FFN                    1,282,688       13.4\n",
      "07: + Residual               1,283,456       10.9\n",
      "08: + 2 Blocks               1,481,216        8.7\n",
      "09: + 2 Heads                1,481,216        8.3\n",
      "10: + MoE                    1,745,152        8.1\n",
      "\n",
      "Lower perplexity = less surprised by correct answer = better.\n",
      "\n",
      "From amoeba (35.8) to specialized mammal (8.1) in 8 notebooks.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "print()\n",
    "print(f\"{'Model':<25} {'Parameters':<15} {'Perplexity':<12}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "for name in models.keys():\n",
    "    params = stats[name]['params']\n",
    "    ppl = stats[name]['final_ppl']\n",
    "    print(f\"{name:<25} {params:>12,}   {ppl:>8.1f}\")\n",
    "\n",
    "print()\n",
    "print(\"Lower perplexity = less surprised by correct answer = better.\")\n",
    "print()\n",
    "print(\"From amoeba (35.8) to specialized mammal (8.1) in 8 notebooks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interactive-header",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Try Your Own Prompts\n",
    "\n",
    "Modify the cell below to test whatever you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "interactive",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03: Bigram (ppl=35.8):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would finally chose a smart and the little girl enjoyed the couch, and it. You made a big smile on her mommy loved to look out. She loved to be careful not so sad and tried to guess the ground in the wood. One day, she also important to chase after a loud voice\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "04: + Attention (ppl=25.0):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would finally chose a smart and loved to take a good job, so proud of the witch.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "05: + Position (ppl=17.7):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  loved to play with her and finding leaves in the garden flowers, and it. One day, so he decided to try new pole and look out. The red and supplies that ended becoming the bush, off the ground in the wood. \n",
      "\n",
      "Jill and Mary felt safe and excited that\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "06: + FFN (ppl=13.4):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would rub her skin and and the little girl enjoyed her delicate little cup happier. She made a big smile on her face and said \"Thank you for being nice and kind. You and this house is happy.\"\n",
      "\n",
      "Molly.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "07: + Residual (ppl=10.9):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would have love to go and play outside with her friends. One day, she saw a pretty so pretty butterfly that she fell in the dirt.\n",
      "\n",
      "S felt sad that ended up her brother had trouble, no matter why.\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "08: + 2 Blocks (ppl=8.7):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would go out looking for and finding things. One day her little girl walked up to the universe so she decided to go on a magical journey.\n",
      "\n",
      "The little girl stepped back and stood her whole way off, she noticed a wood castle that looked like she had never seen before. She asked herself\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "09: + 2 Heads (ppl=8.3):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would finally be looking and and there could take one. One day, it was very hard to explore the forest and wondered what was inside.\n",
      "\n",
      "As the days passed, a little boy decided to share off the ground in a wood kingdom that its home. He went around and tried to change its\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "10: + MoE (ppl=8.1):\n",
      "\n",
      "Prompt: Once upon a time there was a little girl who lived in a magical forest. She\n",
      "Generated:  would go out and go and play all day.\n",
      "\n",
      "One day it was very hard and so she decided to try and find out what was on an adventure. She searched and searched, and saw many trees, flowers in a wood. She was so excited, and she started to laugh. The\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your prompt here!\n",
    "compare_generations(\n",
    "    \"Once upon a time there was a little girl who lived in a magical forest. She\",\n",
    "    max_tokens=60,\n",
    "    temperature=1.0,\n",
    "    seed=42  # Set to None for random each time\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workshop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
